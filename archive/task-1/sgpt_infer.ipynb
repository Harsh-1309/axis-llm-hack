{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoModel(\n",
       "  (wte): Embedding(50259, 768)\n",
       "  (wpe): Embedding(2048, 768)\n",
       "  (drop): Dropout(p=0.0, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): GPTNeoBlock(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPTNeoAttention(\n",
       "        (attention): GPTNeoSelfAttention(\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPTNeoMLP(\n",
       "        (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get our models - The package will take care of downloading the models automatically\n",
    "# For best performance: Muennighoff/SGPT-5.8B-weightedmean-msmarco-specb-bitfit\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\")\n",
    "model = AutoModel.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\")\n",
    "# Deactivate Dropout (There is no dropout in the above models so it makes no difference here but other SGPT models may have dropout)\n",
    "# model.load_state_dict(torch.load('models/sgpt-bi/model_1.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "queries = [\n",
    "    \"software engineer\",\n",
    "]\n",
    "\n",
    "docs = [\n",
    "    \"software engineer c, c++, java, uml, xamp, agile. defence/communications our client is a successful and expanding company developing air defence systems, information systems, targeting systems, communication systems. they now require an additional software engineer to support the design evolution from requirements into software code. identification of test requirements and the development of component test harnesses. supporting design and code review activities to derisk the design evolution qualifications for the software engineer. degree in software engineering, computer science, maths, physics or equivalent good degree pref 1st or ****:1, meng or equiv. essential requirements: software engineer experience in one or more of the following: object oriented analysis and design (preferably  uml)  unix /linux or windows, c, c++, java. html, php candidates from a defence background experience of agile development an advantage. experience of xamp technologies useful. desirable requirements:  software engineer  experience in one or more of the following: realtime design. mathematical or algorithm coding  network protocols  oo case tools  structured design. methods  data and voice networks  computer telephony integration (cti) an appreciation of the following: testing techniques and strategies  configuration management personal attributes flexibility, adaptability, team player, good communication skills, discipline the company provide excellent career prospects and career development recent software graduates will also be considered for these roles. keywords software engineer, c, c++, java, realtime, embedded, uml, linux, unix, oo, agile, xamp. defence, communication systems. salary ****k****k location fleet, hampshire\",\n",
    "    \"user experience developer our client requires a user experience developer to help develop and maintain new and existing projects. the successful candidate will report directly to the technical lead. the post will be based in the farnham, surrey area. applicants applicants will have 1**** years flex development experience and should be able to provide a portfolio of their work. successful applicants will be those seeking a challenging opportunity with an exciting and expanding organisation and who are enthusiastic, flexible and personable, able to work well on both an individual basis and as part of a development team. the positions require good communication skills, both oral and written. specific technical skills/knowledge actionscript 3 flex adobe air flashbuilder basic technical skills/knowledge oo programming and/or mvc framework (highly desirable) software design using uml (desirable) test driven development (highly desirable) version control (desirable) other skills/knowledge of interest javascript, html & css, php/.net/python ios/android process user experience developer location: farnham, surrey salary up to ****\",\n",
    "    \"a leading ecommerce agency is looking to hire a web developer to join their team of ecommerce developers. the role will involve working on different types of client and a candidate should have the ability to create and implement unique, engaging, and intuitive work to meet client requirements. requirements: 34 years of experience in web development **** or more years on ecommerce projects in an agency setting development skills in the following technologies: o asp.net and asp o vb and vb.net o sql server / tsql o com/dcom o xml, html o javascript knowledge of microsoft commerce server, any other ecommerce solutions if this sounds like a challenge that you are ready to take up, send in your cv now\",\n",
    "    \"a successful software organisation based in cheshire is on the lookout for a skilled and experienced software developer to join their technical team. you will be involved in the full software development lifecycle from design to implementation and maintaining current applications and websites. qualifications: minimum of 2 years commercial development experience using; vb.net, c.net, ms sql experience with the following technologies is a must: o visual studio (**** and 2010) o sql management studio o visual sourcesafe/team foundation server should have a working experience or knowledge of the following: o asp.net o ajax, javascript, css, html, xml o web services, tsql, sql scripting / stored procedures this is a great opportunity for you to be a part of a well known leader in financial software. if you think you are able to take on a new challenge, send your cv now\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECB_QUE_BOS = tokenizer.encode(\"[\", add_special_tokens=False)[0]\n",
    "SPECB_QUE_EOS = tokenizer.encode(\"]\", add_special_tokens=False)[0]\n",
    "\n",
    "SPECB_DOC_BOS = tokenizer.encode(\"{\", add_special_tokens=False)[0]\n",
    "SPECB_DOC_EOS = tokenizer.encode(\"}\", add_special_tokens=False)[0]\n",
    "\n",
    "\n",
    "def tokenize_with_specb(texts, is_query):\n",
    "    # Tokenize without padding\n",
    "    batch_tokens = tokenizer(texts, padding=False, truncation=True)   \n",
    "    # Add special brackets & pay attention to them\n",
    "    for seq, att in zip(batch_tokens[\"input_ids\"], batch_tokens[\"attention_mask\"]):\n",
    "        if is_query:\n",
    "            seq.insert(0, SPECB_QUE_BOS)\n",
    "            seq.append(SPECB_QUE_EOS)\n",
    "        else:\n",
    "            seq.insert(0, SPECB_DOC_BOS)\n",
    "            seq.append(SPECB_DOC_EOS)\n",
    "        att.insert(0, 1)\n",
    "        att.append(1)\n",
    "    # Add padding\n",
    "    batch_tokens = tokenizer.pad(batch_tokens, padding=True, return_tensors=\"pt\")\n",
    "    return batch_tokens\n",
    "\n",
    "def get_weightedmean_embedding(batch_tokens, model):\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        # Get hidden state of shape [bs, seq_len, hid_dim]\n",
    "        last_hidden_state = model(**batch_tokens, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "\n",
    "    # Get weights of shape [bs, seq_len, hid_dim]\n",
    "    weights = (\n",
    "        torch.arange(start=1, end=last_hidden_state.shape[1] + 1)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float().to(last_hidden_state.device)\n",
    "    )\n",
    "\n",
    "    # Get attn mask of shape [bs, seq_len, hid_dim]\n",
    "    input_mask_expanded = (\n",
    "        batch_tokens[\"attention_mask\"]\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "    # Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -> bs, hidden_dim\n",
    "    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded * weights, dim=1)\n",
    "    sum_mask = torch.sum(input_mask_expanded * weights, dim=1)\n",
    "\n",
    "    embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embeddings = get_weightedmean_embedding(tokenize_with_specb(queries, is_query=True), model)\n",
    "doc_embeddings = get_weightedmean_embedding(tokenize_with_specb(docs, is_query=False), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between \"software engineer\" and \"software engineer c,...\" is: 0.684\n",
      "Cosine similarity between \"software engineer\" and \"user experience deve...\" is: 0.386\n",
      "Cosine similarity between \"software engineer\" and \"a leading ecommerce ...\" is: 0.589\n"
     ]
    }
   ],
   "source": [
    "# Calculate cosine similarities\n",
    "# Cosine similarities are in [-1, 1]. Higher means more similar\n",
    "cosine_sim_0_1 = 1 - cosine(query_embeddings[0], doc_embeddings[0])\n",
    "cosine_sim_0_2 = 1 - cosine(query_embeddings[0], doc_embeddings[1])\n",
    "cosine_sim_0_3 = 1 - cosine(query_embeddings[0], doc_embeddings[3])\n",
    "\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (queries[0], docs[0][:20] + \"...\", cosine_sim_0_1))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (queries[0], docs[1][:20] + \"...\", cosine_sim_0_2))\n",
    "print(\"Cosine similarity between \\\"%s\\\" and \\\"%s\\\" is: %.3f\" % (queries[0], docs[2][:20] + \"...\", cosine_sim_0_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75ba99cab82ae3d1b68f6c82e3eb6569fe8478ff7a1b1bd0fcbd81a78982ddb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
