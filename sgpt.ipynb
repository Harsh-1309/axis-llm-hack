{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sentence_transformers import SentenceTransformer, SentencesDataset, losses, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_metric_learning import losses\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from sentence_transformers.readers import InputExample\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "from accelerate import Accelerator\n",
    "from accelerate import notebook_launcher\n",
    "import os\n",
    "import random\n",
    "import hnswlib\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"runs/sgpt-bi\")\n",
    "\n",
    "# df_train=pd.read_csv(\"nfcorpus/final_data_train.csv\")\n",
    "# df_test=pd.read_csv(\"nfcorpus/final_data_test.csv\")\n",
    "\n",
    "df=pd.read_csv(\"jd_pos_neg.csv\")\n",
    "\n",
    "# print(df)\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, label, query, doc):\n",
    "        self.labels = label\n",
    "        self.query = query\n",
    "        self.doc = doc\n",
    "    def __len__(self):\n",
    "            return len(self.query)\n",
    "    def __getitem__(self, idx):\n",
    "            label = self.labels[idx]\n",
    "            query = self.query[idx]\n",
    "            doc = self.doc[idx]\n",
    "            sample = {\"Query\": query, \"Doc\": doc, \"Label\": label}\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "TD_train = CustomTextDataset(train_df['label'],train_df['title'],train_df['desc'])\n",
    "DL_DS_train = DataLoader(TD_train, batch_size=16,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "TD_val = CustomTextDataset(val_df['label'],val_df['title'],val_df['desc'])\n",
    "DL_DS_val = DataLoader(TD_val, batch_size=16,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(DL_DS_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_docs=list(docs_df_test.Doc)\n",
    "# all_docs_ids=list(docs_df_test.Did)\n",
    "# all_queries=list(queries_df_test.Query)\n",
    "# all_queries_ids=list(queries_df_test.Qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metrics(k,query_emb,doc_emb,all_queries_ids,all_docs_ids,rel_df):\n",
    "\n",
    "    mrr_scores={}\n",
    "    ndcg_scores={}\n",
    "    pr={}\n",
    "    re={}\n",
    "    f1={}\n",
    "    mrr_k=[10,20]\n",
    "    ndcg_k=[10,30,50,100]\n",
    "    prf_k=[10,30,50,100]\n",
    "\n",
    "    ids=np.arange(len(doc_emb))\n",
    "    p = hnswlib.Index(space = 'cosine', dim = len(doc_emb[0]))\n",
    "    p.init_index(max_elements = len(doc_emb), ef_construction = 300, M = 32)\n",
    "    p.add_items(doc_emb, ids)\n",
    "    p.set_ef(350)\n",
    "    \n",
    "    def ndcg(true_relevance, pred_relevance, k_):\n",
    "        if k_ is not None:\n",
    "            true_relevance = true_relevance[:k_]\n",
    "            pred_relevance = pred_relevance[:k_]\n",
    "\n",
    "        dcg = np.sum((2 ** np.array(true_relevance) - 1) / np.log2(np.arange(2, len(true_relevance) + 2)))\n",
    "        idcg = np.sum((2 ** np.sort(np.array(true_relevance))[::-1] - 1) / np.log2(np.arange(2, len(true_relevance) + 2)))\n",
    "\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    for i in range(len(all_queries_ids)):\n",
    "        labels, distances = p.knn_query(query_emb[i], k = k)\n",
    "\n",
    "        ## get doc ids\n",
    "        ann_ids=[]\n",
    "        for j in labels[0]:\n",
    "            ann_ids.append(all_docs_ids[j])\n",
    "\n",
    "        positive_samples=list(rel_df[rel_df.Query==all_queries_ids[i]].Doc)\n",
    "\n",
    "        ## MRR\n",
    "        for n in mrr_k:\n",
    "            positive_sample_ranks=[]\n",
    "            for an in ann_ids[:n]:\n",
    "                if an in positive_samples:\n",
    "                    positive_sample_ranks.append(ann_ids[:n].index(an)+1)\n",
    "                    break\n",
    "                else:\n",
    "                    positive_sample_ranks.append(0)\n",
    "            if n not in mrr_scores.keys():\n",
    "                if positive_sample_ranks[0]!=0:\n",
    "                    mrr_scores[n] = 1/positive_sample_ranks[0]\n",
    "            else:\n",
    "                if positive_sample_ranks[0]!=0:\n",
    "                    mrr_scores[n] += 1/positive_sample_ranks[0]\n",
    "\n",
    "        pred_score=[1-x for x in distances[0]]\n",
    "        true_scores=[]\n",
    "        \n",
    "\n",
    "        ## nDCG\n",
    "        for ann in ann_ids:\n",
    "            if ann in positive_samples:\n",
    "                true_scores.append(1)\n",
    "            else:\n",
    "                true_scores.append(0)\n",
    "\n",
    "        for n in ndcg_k:\n",
    "            # temp_scores=[]\n",
    "            # temp_scores.append(ndcg(true_scores,pred_score,n))\n",
    "            if n not in ndcg_scores.keys():\n",
    "                ndcg_scores[n]=ndcg(true_scores,pred_score,n)\n",
    "            else:\n",
    "                ndcg_scores[n]+=ndcg(true_scores,pred_score,n)\n",
    "\n",
    "        ## PRF\n",
    "        for n in prf_k:\n",
    "            count=0 \n",
    "            # temp_p=[]\n",
    "            temp_r=[]\n",
    "            for ann in ann_ids[:n]:\n",
    "                if ann in positive_samples:\n",
    "                    count+=1\n",
    "\n",
    "            if n not in pr.keys():\n",
    "                pr[n]=count/n\n",
    "            else:\n",
    "                pr[n]+=count/n\n",
    "\n",
    "            if n not in re.keys():\n",
    "                re[n]=count/len(positive_samples)\n",
    "            else:\n",
    "                re[n]+=count/len(positive_samples)\n",
    "    \n",
    "    for n in mrr_k:\n",
    "        try:\n",
    "            mrr_scores[n]/=len(all_queries_ids)\n",
    "        except KeyError:\n",
    "            continue\n",
    "    for n in ndcg_k:\n",
    "        ndcg_scores[n]/=len(all_queries_ids)\n",
    "    for n in prf_k:\n",
    "        pr[n]/=len(all_queries_ids)\n",
    "        re[n]/=len(all_queries_ids)\n",
    "        f1[n]=2/((1/pr[n])+(1/re[n]))\n",
    "    \n",
    "    return mrr_scores, ndcg_scores, pr, re, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\")\n",
    "model = AutoModel.from_pretrained(\"Muennighoff/SGPT-125M-weightedmean-msmarco-specb-bitfit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "SPECB_QUE_BOS = tokenizer.encode(\"[\", add_special_tokens=False)[0]\n",
    "SPECB_QUE_EOS = tokenizer.encode(\"]\", add_special_tokens=False)[0]\n",
    "\n",
    "SPECB_DOC_BOS = tokenizer.encode(\"{\", add_special_tokens=False)[0]\n",
    "SPECB_DOC_EOS = tokenizer.encode(\"}\", add_special_tokens=False)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_specb(texts, is_query):\n",
    "    # Tokenize without padding\n",
    "    batch_tokens = tokenizer(texts, padding=False, truncation=True, max_length=256)   \n",
    "    # Add special brackets & pay attention to them\n",
    "    for seq, att in zip(batch_tokens[\"input_ids\"], batch_tokens[\"attention_mask\"]):\n",
    "        if is_query:\n",
    "            seq.insert(0, SPECB_QUE_BOS)\n",
    "            seq.append(SPECB_QUE_EOS)\n",
    "        else:\n",
    "            seq.insert(0, SPECB_DOC_BOS)\n",
    "            seq.append(SPECB_DOC_EOS)\n",
    "        att.insert(0, 1)\n",
    "        att.append(1)\n",
    "    # Add padding\n",
    "    batch_tokens = tokenizer.pad(batch_tokens, padding=True, return_tensors=\"pt\")\n",
    "    return batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weightedmean_embedding(batch_tokens, model, state=\"train\"):\n",
    "    # Get the embeddings\n",
    "    # with torch.no_grad():\n",
    "        # Get hidden state of shape [bs, seq_len, hid_dim]\n",
    "    # model=model.to(device).half()\n",
    "    if state==\"train\":\n",
    "        last_hidden_state = model(**batch_tokens, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "    else:\n",
    "        batch_tokens.to('cuda')\n",
    "        with torch.no_grad():\n",
    "            last_hidden_state = model(**batch_tokens, output_hidden_states=True, return_dict=True).last_hidden_state\n",
    "\n",
    "    # print(last_hidden_state)\n",
    "    # Get weights of shape [bs, seq_len, hid_dim]\n",
    "    weights = (\n",
    "        torch.arange(start=1, end=last_hidden_state.shape[1] + 1)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float().to(last_hidden_state.device)\n",
    "    )\n",
    "\n",
    "    # Get attn mask of shape [bs, seq_len, hid_dim]\n",
    "    input_mask_expanded = (\n",
    "        batch_tokens[\"attention_mask\"]\n",
    "        .unsqueeze(-1)\n",
    "        .expand(last_hidden_state.size())\n",
    "        .float()\n",
    "    )\n",
    "\n",
    "    # Perform weighted mean pooling across seq_len: bs, seq_len, hidden_dim -> bs, hidden_dim\n",
    "    sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded * weights, dim=1)\n",
    "    sum_mask = torch.sum(input_mask_expanded * weights, dim=1)\n",
    "\n",
    "    embeddings = sum_embeddings / sum_mask\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=5\n",
    "model_save_path=\"models/sgpt-bi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_exp(q,d,t=10):\n",
    "    q = q.unsqueeze(0)\n",
    "    d = d.unsqueeze(0)\n",
    "    return torch.exp(t * cosine_similarity(q, d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model=model,optimizer=optimizer,data_train=DL_DS_train):\n",
    "    # loss=torch.nn.CosineEmbeddingLoss()\n",
    "    # loss=losses.SupConLoss(temperature=10)\n",
    "    # loss=ContrastiveLoss(margin=1)\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    model, optimizer, DL_DS_train = accelerator.prepare(model, optimizer, data_train)\n",
    "\n",
    "    \n",
    "    # model.to(device)\n",
    "    model.train()\n",
    "    print_every=5\n",
    "    evaluate_after=500\n",
    "    query_infer_batch=50\n",
    "    doc_infer_batch=50\n",
    "    model_loc = \"models/sgpt-bi\"\n",
    "    if not os.path.isdir(model_loc):\n",
    "        os.makedirs(model_loc)\n",
    "\n",
    "    step = 0\n",
    "    running_loss=0.0\n",
    "    for e in tqdm(range(1,num_epochs+1)):\n",
    "        print(f\"training epoch {e}\")\n",
    "        model.train()\n",
    "        for _, data_ in enumerate(tqdm(DL_DS_train)):\n",
    "            step += 1\n",
    "            optimizer.zero_grad()       \n",
    "\n",
    "            query_=[x for x in data_[\"Query\"]]\n",
    "            doc_=[x for x in data_[\"Doc\"]]\n",
    "            label_=[x for x in data_[\"Label\"]]\n",
    "            \n",
    "            query_tokens=tokenize_with_specb(query_, is_query=True).to(accelerator.device)\n",
    "            doc_tokens=tokenize_with_specb(doc_, is_query=False).to(accelerator.device) #td\n",
    "\n",
    "            query_embed=get_weightedmean_embedding(query_tokens,model,\"train\")\n",
    "            doc_embed=get_weightedmean_embedding(doc_tokens,model,\"train\")\n",
    "\n",
    "            query_embed=nn.functional.normalize(query_embed, dim=1)\n",
    "            doc_embed=nn.functional.normalize(doc_embed, dim=1)\n",
    "            # print(query_embed[0])\n",
    "            \n",
    "            similarity_scores = torch.mm(query_embed, doc_embed.t())\n",
    "            # batch_loss=0.0\n",
    "            # Calculate cross-entropy loss\n",
    "            loss = F.cross_entropy(similarity_scores, label_)\n",
    "            \n",
    "            ## extract hard \n",
    "\n",
    "            batch_loss=loss.mean()\n",
    "\n",
    "            accelerator.backward(batch_loss)\n",
    "            optimizer.step()\n",
    "\n",
    "            # _, predicted = torch.max(similarity_scores.data, 1)\n",
    "            # correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            running_loss += batch_loss.item()\n",
    "            if (step % print_every) == 0:\n",
    "                writer.add_scalar(\"training loss\",running_loss/print_every,step)\n",
    "                running_loss=0.0\n",
    "\n",
    "            if (step % evaluate_after) ==0:\n",
    "                print(f\"evaluating at step {step}\")\n",
    "\n",
    "                model.eval()\n",
    "                total_val_loss = 0.0\n",
    "                correct_val_predictions = 0\n",
    "                with torch.no_grad():\n",
    "                    # for batch in DL_DS_val:\n",
    "                    step_val=0\n",
    "                    running_loss_val=0.0\n",
    "                    for _, data_val in enumerate(tqdm(DL_DS_val)):\n",
    "                        step_val+=1\n",
    "                        query_val=[x for x in data_val[\"Query\"]]\n",
    "                        doc_val=[x for x in data_val[\"Doc\"]]\n",
    "                        label_val=[x for x in data_val[\"Label\"]]\n",
    "\n",
    "                        query_tokens_val=tokenize_with_specb(query_val, is_query=True).to(accelerator.device)\n",
    "                        doc_tokens_val=tokenize_with_specb(doc_val, is_query=False).to(accelerator.device) #td\n",
    "\n",
    "                        query_embed_val=get_weightedmean_embedding(query_tokens_val,model,\"test\")\n",
    "                        doc_embed_val=get_weightedmean_embedding(doc_tokens_val,model,\"test\")\n",
    "\n",
    "                        query_embed_val=nn.functional.normalize(query_embed_val, dim=1)\n",
    "                        doc_embed_val=nn.functional.normalize(doc_embed_val, dim=1)\n",
    "\n",
    "                        similarity_scores_val = torch.mm(query_embed_val, doc_embed_val.t())\n",
    "                        # batch_loss=0.0\n",
    "                        # Calculate cross-entropy loss\n",
    "                        loss_val = F.cross_entropy(similarity_scores_val, label_val)\n",
    "                        loss_val=loss_val.mean()\n",
    "\n",
    "                        _, val_predicted = torch.max(similarity_scores_val.data, 1)\n",
    "                        correct_val_predictions += (val_predicted == label_val).sum().item()\n",
    "\n",
    "                        running_loss_val += loss_val.item()\n",
    "                        if (step_val % print_every) == 0:\n",
    "                            writer.add_scalar(\"validation loss\",running_loss/print_every,step_val)\n",
    "                            running_loss_val=0.0\n",
    "                # avg_val_loss = loss_val.mean()\n",
    "                val_accuracy = correct_val_predictions.mean()\n",
    "                writer.add_scalar(\"Validation accuracy\",val_accuracy,step_val)\n",
    " \n",
    "        print(f\"Model saved at - {model_save_path}/model_{e}.pt\")\n",
    "        torch.save(model.state_dict(), f\"{model_save_path}/model_{e}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/816 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _, data_ in enumerate(tqdm(DL_DS_train)):\n",
    "    query_=[x for x in data_[\"Label\"]]\n",
    "    print(query_)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('pyt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75ba99cab82ae3d1b68f6c82e3eb6569fe8478ff7a1b1bd0fcbd81a78982ddb8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
